{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "import requests\n",
    "import pattern3.web as web\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\" This little code extracts all the links in the comments of a webpage.\"\n",
    "\" Written originally for hacker news threads, can be used for similar \"\n",
    "\" websites such as reddit as well\"\n",
    "\" With BeautifulSoup \" \n",
    "    \n",
    "def get_links0(webpage):\n",
    "    \n",
    "    url = web.URL(webpage)\n",
    "    bs = BeautifulSoup(url.download(cached = False)) \n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    for comment in bs.findAll('div', 'comment'):\n",
    "        if comment.find('a') is not None:\n",
    "            links_bycomment = comment.findAll('a', href=True)\n",
    "            for link in links_bycomment:\n",
    "                link_content = link['href']\n",
    "                links.append(link_content)\n",
    "                print (link_content)\n",
    "            print (\"\")\n",
    "                \n",
    "    \n",
    "    return list(set(links)) # removes duplicates \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\" This little code extracts all the links on the main article of a webpage.\"\n",
    "\" With BeautifulSoup \" \n",
    "    \n",
    "def get_links2(webpage):\n",
    "    \n",
    "    url = web.URL(webpage)\n",
    "    bs = BeautifulSoup(url.download(cached = False)) \n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    for paragraph in bs.findAll('p'):\n",
    "        if paragraph.find('a') is not None:\n",
    "            links_byparagraph = paragraph.findAll('a', href=True)\n",
    "            for link in links_byparagraph:\n",
    "                link_content = link['href']\n",
    "                links.append(link_content)\n",
    "                print (link_content)\n",
    "            print (\"\")\n",
    "                \n",
    "    \n",
    "    return list(set(links)) # removes duplicates \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/odeniz/Downloads/ENTER/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/odeniz/Downloads/ENTER/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.manning.com/books/taming-text\n",
      "\n",
      "http://garysieling.com/blog/entity-recognition-with-scala-and-stanford-nlp-named-entity-recognizer\n",
      "http://tika.apache.org\n",
      "http://www.nltk.org\n",
      "http://edeferia.blogspot.com/2015/03/from-natural-language-to-calendar.html\n",
      "https://www.coursera.org/learn/natural-language-processing\n",
      "\n",
      "https://www.amazon.com/Natural-Language-Processing-Python-Analyzing/dp/0596516495/ref=sr_1_1?ie=UTF8&qid=1478777763&sr=8-1&keywords=natural+language+processing+with+python\n",
      "\n",
      "http://www.nltk.org/book/\n",
      "\n",
      "https://www.youtube.com/watch?v=nfoudtpBV68&list=PL6397E4B26D00A269\n",
      "https://see.stanford.edu/Course/CS229\n",
      "https://www.youtube.com/playlist?list=PLIiVRB6G_w0i-uOoS6cDh_5nkUyxy_hxe\n",
      "\n",
      "https://web.stanford.edu/~jurafsky/slp3/\n",
      "\n",
      "http://www.nltk.org\n",
      "https://github.com/vseloved/cl-nlp\n",
      "\n",
      "http://hackaday.com/2016/06/08/talking-star-trek/\n",
      "https://scaryreasoner.wordpress.com/2016/05/14/speech-recognition-and-natural-language-processing-in-space-nerds-in-space/\n",
      "\n",
      "http://academictorrents.com/details/f99e7184fca947ee8f77901679e171fcadbf82e7\n",
      "https://www.youtube.com/playlist?list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL\n",
      "\n",
      "http://nltk.org\n",
      "http://nltk.org/book\n",
      "\n",
      "https://spacy.io/\n",
      "\n",
      "http://explosion.ai\n",
      "\n",
      "https://medium.com/learning-new-stuff/machine-learning-in-a-week-a0da25d59850#.y8pe9o9qm\n",
      "\n",
      "https://web.stanford.edu/~jurafsky/slp3/\n",
      "http://nlp.stanford.edu/fsnlp/\n",
      "\n",
      "http://stanfordnlp.github.io/CoreNLP/\n",
      "\n",
      "http://corenlp.run\n",
      "\n",
      "https://github.com/andrewt3000/dl4nlp\n",
      "\n",
      "https://karczmarczuk.users.greyc.fr/TEACH/TAL/Doc/Handbook%20Of%20Natural%20Language%20Processing,%20Second%20Edition%20Chapman%20&%20Hall%20Crc%20Machine%20Learning%20&%20Pattern%20Recognition%202010.pdf\n",
      "\n",
      "https://github.com/pickhardt/betty\n",
      "http://iftechfoundation.org/frequently-asked-questions/\n",
      "\n",
      "https://egghead.io/lessons/node-js-break-up-language-strings-into-parts-using-natural\n",
      "\n",
      "https://www.findlectures.com\n",
      "\n",
      "https://github.com/keonkim/awesome-nlp\n",
      "https://lekta.ai/blog/\n",
      "\n",
      "http://mt-class.org/penn/\n",
      "http://www.statmt.org/book/\n",
      "\n",
      "https://www.amazon.com/gp/aw/d/364241463X/ref=dp_ob_neva_mobile\n",
      "\n",
      "https://www.quora.com/How-do-I-learn-Natural-Language-Processing\n",
      "\n",
      "http://www.mtome.com/Publications/PNLA/prolog-digital.pdf\n",
      "\n",
      "https://sigai.acm.org/index.html\n",
      "\n",
      "http://nlp.stanford.edu/IR-book/\n",
      "\n",
      "http://blog.algorithmia.com/introduction-natural-language-processing-nlp/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://sigai.acm.org/index.html',\n",
       " 'http://corenlp.run',\n",
       " 'https://lekta.ai/blog/',\n",
       " 'http://garysieling.com/blog/entity-recognition-with-scala-and-stanford-nlp-named-entity-recognizer',\n",
       " 'http://tika.apache.org',\n",
       " 'https://scaryreasoner.wordpress.com/2016/05/14/speech-recognition-and-natural-language-processing-in-space-nerds-in-space/',\n",
       " 'https://www.youtube.com/playlist?list=PLIiVRB6G_w0i-uOoS6cDh_5nkUyxy_hxe',\n",
       " 'http://www.mtome.com/Publications/PNLA/prolog-digital.pdf',\n",
       " 'http://nlp.stanford.edu/IR-book/',\n",
       " 'http://iftechfoundation.org/frequently-asked-questions/',\n",
       " 'http://nltk.org/book',\n",
       " 'http://mt-class.org/penn/',\n",
       " 'http://nltk.org',\n",
       " 'https://www.youtube.com/playlist?list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL',\n",
       " 'https://github.com/pickhardt/betty',\n",
       " 'http://nlp.stanford.edu/fsnlp/',\n",
       " 'https://web.stanford.edu/~jurafsky/slp3/',\n",
       " 'https://github.com/keonkim/awesome-nlp',\n",
       " 'https://www.quora.com/How-do-I-learn-Natural-Language-Processing',\n",
       " 'http://stanfordnlp.github.io/CoreNLP/',\n",
       " 'https://karczmarczuk.users.greyc.fr/TEACH/TAL/Doc/Handbook%20Of%20Natural%20Language%20Processing,%20Second%20Edition%20Chapman%20&%20Hall%20Crc%20Machine%20Learning%20&%20Pattern%20Recognition%202010.pdf',\n",
       " 'https://see.stanford.edu/Course/CS229',\n",
       " 'https://egghead.io/lessons/node-js-break-up-language-strings-into-parts-using-natural',\n",
       " 'https://github.com/vseloved/cl-nlp',\n",
       " 'http://academictorrents.com/details/f99e7184fca947ee8f77901679e171fcadbf82e7',\n",
       " 'https://www.coursera.org/learn/natural-language-processing',\n",
       " 'https://medium.com/learning-new-stuff/machine-learning-in-a-week-a0da25d59850#.y8pe9o9qm',\n",
       " 'http://explosion.ai',\n",
       " 'https://www.manning.com/books/taming-text',\n",
       " 'http://blog.algorithmia.com/introduction-natural-language-processing-nlp/',\n",
       " 'https://www.amazon.com/gp/aw/d/364241463X/ref=dp_ob_neva_mobile',\n",
       " 'http://edeferia.blogspot.com/2015/03/from-natural-language-to-calendar.html',\n",
       " 'http://www.nltk.org',\n",
       " 'http://hackaday.com/2016/06/08/talking-star-trek/',\n",
       " 'http://www.nltk.org/book/',\n",
       " 'https://www.findlectures.com',\n",
       " 'https://www.amazon.com/Natural-Language-Processing-Python-Analyzing/dp/0596516495/ref=sr_1_1?ie=UTF8&qid=1478777763&sr=8-1&keywords=natural+language+processing+with+python',\n",
       " 'http://www.statmt.org/book/',\n",
       " 'https://spacy.io/',\n",
       " 'https://github.com/andrewt3000/dl4nlp',\n",
       " 'https://www.youtube.com/watch?v=nfoudtpBV68&list=PL6397E4B26D00A269']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" Ask HN: How Can I Get into NLP (Natural Language Processing)? \"\n",
    "mypage = 'https://news.ycombinator.com/item?id=12916498'\n",
    "get_links0(mypage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print (len(mylinks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><Br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://en.wikipedia.org/wiki/Bag-of-words_model\n",
      "\n",
      "http://www.kaggle.com/c/titanic-gettingStarted\n",
      "\n",
      "http://www.nltk.org/book/\n",
      "\n",
      "https://github.com/wendykan/DeepLearningMovies/blob/master/BagOfWords.py\n",
      "\n",
      "http://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
      "\n",
      "https://docs.python.org/2/library/re.html#\n",
      "\n",
      "http://en.wikipedia.org/wiki/Tokenization\n",
      "\n",
      "http://en.wikipedia.org/wiki/Stop_words\n",
      "http://www.nltk.org\n",
      "http://www.nltk.org/install.html\n",
      "\n",
      "https://docs.python.org/2/howto/unicode.html#python-2-x-s-unicode-support\n",
      "\n",
      "http://en.wikipedia.org/wiki/Bag-of-words_model\n",
      "\n",
      "http://scikit-learn.org/stable/install.html\n",
      "\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
      "\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
      "\n",
      "http://blog.kaggle.com/2012/07/06/the-dangers-of-overfitting-psychopathy-post-mortem/\n",
      "\n",
      "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/odeniz/Downloads/ENTER/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/odeniz/Downloads/ENTER/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://www.kaggle.com/c/titanic-gettingStarted',\n",
       " 'http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html',\n",
       " 'http://www.crummy.com/software/BeautifulSoup/bs4/doc/',\n",
       " 'http://en.wikipedia.org/wiki/Stop_words',\n",
       " 'http://www.nltk.org/install.html',\n",
       " 'https://docs.python.org/2/howto/unicode.html#python-2-x-s-unicode-support',\n",
       " 'http://scikit-learn.org/stable/install.html',\n",
       " 'http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html',\n",
       " 'https://docs.python.org/2/library/re.html#',\n",
       " 'http://en.wikipedia.org/wiki/Bag-of-words_model',\n",
       " 'http://en.wikipedia.org/wiki/Tokenization',\n",
       " 'http://www.nltk.org',\n",
       " 'https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews',\n",
       " 'http://www.nltk.org/book/',\n",
       " 'https://github.com/wendykan/DeepLearningMovies/blob/master/BagOfWords.py',\n",
       " 'http://blog.kaggle.com/2012/07/06/the-dangers-of-overfitting-psychopathy-post-mortem/']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylink3 = 'https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words'\n",
    "get_links2(mylink3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
